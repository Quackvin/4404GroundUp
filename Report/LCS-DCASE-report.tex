\documentclass[11pt]{article}

% Packages
\usepackage[affil-it]{authblk}		% author affiliations in title
\usepackage[margin=1in]{geometry}	% one inch margins
\usepackage{enumerate}				% numbered list environment
\usepackage{wrapfig}				% text wrapped figures
\usepackage{graphicx}				% figures, better than "graphics" apparently
\usepackage{subcaption}				% captions for sub-figures
\usepackage{amsmath}				% math stuff
\usepackage{fancyhdr}				% custom headers
\usepackage[numbers]{natbib}		% used for citet and other citation formats
\usepackage{booktabs}				% nice table borders
\usepackage{tabularx}				% equal width table columns
\usepackage[hidelinks]{hyperref} 	% puts click-able links in the text, fixes issue with urls that have underscores
\usepackage[defaultlines=2,all]{nowidow}	% prevents orphan and widow lines at start and end of paragraphs
\usepackage{multicol}				% control over using multiple columns
\usepackage{lipsum}					% dummy text

% To-do notes and to-do list
\usepackage[colorinlistoftodos,prependcaption]{todonotes}

% Width between columns
\setlength{\columnsep}{0.8cm}

% Tables with centred, fixed with columns
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}		% used in cover page
\newcolumntype{Y}{>{\centering\arraybackslash}X}			% used in tabularx for even column distribution

% Make "References" appear in the table of contents
\usepackage[nottoc]{tocbibind}
\renewcommand{\tocbibname}{References}

% Headers and footers
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rhead{CITS4404 Artificial Intelligence \& Adaptive Systems}
\lhead{Team G1}
\setlength{\headheight}{14pt}
\cfoot{\thepage}

% Nice abstract
\renewenvironment{abstract}
{\small
	\begin{center}
		\bfseries \abstractname\vspace{-.5em}\vspace{0pt}
	\end{center}
	\list{}{
		\setlength{\leftmargin}{.5cm}%
		\setlength{\rightmargin}{\leftmargin}%
	}%
	\item\relax}
{\endlist}



\begin{document}

% Title
\title{
	Applying Learning Classifier Systems to Acoustic Scene Classification: DCASE 2017 Challenge \\
	\vspace{0.2in}
	\large CITS4404 Artificial Intelligence \& Adaptive Systems Team Project
}
\author{Yiyang~Gao~(21263128)}
\author{Aaron~Hurst~(21325887)}
\author{Kevin~Kuek~(21307006)}
\author{Scott~McCormack~(21875529)}
\affil{School of Computer Science and Software Engineering}

\date{3rd November, 2017}

\maketitle

% Abstract
\begin{abstract}
	This will be our abstract \\
	\lipsum[2]
\end{abstract}

\begin{multicols}{2}


\section{Introduction}


\lipsum[1]




\section{Background}

This section provides a brief review of learning classifier systems (\ref{sec:LCS}), the DCASE Challenge (\ref{sec:DCASE}) and acoustic scene classification (\ref{sec:ASC}).



\subsection{Learning Classifier Systems}
\label{sec:LCS}

First introduced in the mid-1970s, Learning Classifier Systems (LCSs) are a rule-based machine leaning algorithm with a unique combination of learning mechanisms \cite{Butz2015}. The core of LCSs is the population of rules, or \textit{classifiers}, which collectively form the solution to the given problem \cite{Urbanowicz2009}. This population of classifiers is gradually evolved toward an optimal and \textit{optimally general} set \cite{Urbanowicz2009}.

The motivation for this structure is that, when modelling and attempting to predict the outcome of complex systems, a desirable approach is to develop a distributed population of classifiers -- or rules -- that together form an accurate model \cite[p.~2]{Urbanowicz2009}. Each classifier, then, spans a subspace of the problem, with the population spanning the entire problem.

Individual classifiers consist of a condition-action rule which says: 'If a problem instance matches this condition, perform this action'.


, an action and a number of parameters. The condition specifies the subspace of the problem, while the action proposes an outcome for this subspace. For a given \textit{instance} of the problem, the classifiers then say: `If the problem instance matches


XCS can be distinguished by the
following key features: an accuracy based fitness, a niche
GA (acting in the action set [A]), and an adaptation of
standard Q-Learning as credit assignment.




evolved using a learning mechanism incorporating a number of mechanisms -- GA, subsumption, deletion and covering

online vs reinforcement (key success area of LCS)

continuous vs discrete data

fitness: strength vs. accuracy

Common variants: MCS, XCS, XCSR




\subsection{DCASE Challenge}
\label{sec:DCASE}

what (sound recognition, machine listening), why (motivations) and how (mechanics of challenge)

baseline solution, best results


\subsection{Acoustic Scene Classification}
\label{sec:ASC}

more detailed description of this particular task from the challenge



\section{Experiments}
\label{sec:exp}

blah

\subsection{Modification of Existing Code}
\label{sec:ryan}

\todo[inline]{Is there a better title you'd suggest Scott?}

Description of Ryan's code, how we modified it and the results obtained



\subsection{Adapted XCS(R) Design}
\label{sec:home}

Details of the LCS we made, specific design changes relative to the standard implementation for our problem




%- Feature Extraction, motivation for feature choices (i.e. feautres don't vary a lot across a file (low standard deviation), need to reduce the number of features)
%
%- Description of the code (the one we made outselved and Urbaonwicz's)
%
%- Parameters used
%
%Structure and design of algorithm: generic XCS modified for continuous data and offline learning
%
%Key modifications to basic LCs: (1) added tolerance to subsumption and removed wildcard criteria so that subsumption has some likelihood of occurring, (2) changed deletion criteria so that classifiers with little utility (low match count) can be deleted, motivation was the really low average match count, (3) action set subsumption criteria for most general classifier determined as most wildcards or equal most and largest range, (4) ...
%
%Show graph of function for deletion experience threshold (developed in response to \#2)




\section{Results}


\subsection{Environment Representations}

Alternative feature processing investigated



\subsection{Parameter Tuning}

%- Rate of learning (improvement in accuracy over time)
%
%- Overall results: pairwise, all classes at once (confusion matrices)
%
%
%Theoretical analysis?






\section{Discussion}








\section{Conclusion}









\bibliographystyle{IEEEtranN}
\bibliography{IEEEabrv,References}

\end{multicols}

\end{document}