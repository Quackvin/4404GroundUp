\documentclass[11pt]{article}

% Packages
\usepackage[affil-it]{authblk}		% author affiliations in title
\usepackage[margin=1in]{geometry}	% one inch margins
\usepackage{enumerate}				% numbered list environment
\usepackage{wrapfig}				% text wrapped figures
\usepackage{graphicx}				% figures, better than "graphics" apparently
\usepackage{subcaption}				% captions for sub-figures
\usepackage{amsmath}				% math stuff
\usepackage{fancyhdr}				% custom headers
\usepackage[numbers]{natbib}		% used for citet and other citation formats
\usepackage{booktabs}				% nice table borders
\usepackage{tabularx}				% equal width table columns
\usepackage[hidelinks]{hyperref} 	% puts click-able links in the text, fixes issue with urls that have underscores
\usepackage[defaultlines=2,all]{nowidow}	% prevents orphan and widow lines at start and end of paragraphs
\usepackage{multicol}				% control over using multiple columns
\usepackage{lipsum}					% dummy text

% To-do notes and to-do list
\usepackage[colorinlistoftodos,prependcaption]{todonotes}

% Width between columns
\setlength{\columnsep}{0.8cm}

% Tables with centred, fixed with columns
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}		% used in cover page
\newcolumntype{Y}{>{\centering\arraybackslash}X}			% used in tabularx for even column distribution

% Make "References" appear in the table of contents
\usepackage[nottoc]{tocbibind}
\renewcommand{\tocbibname}{References}

% Headers and footers
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rhead{CITS4404 Artificial Intelligence \& Adaptive Systems}
\lhead{Team G1}
\setlength{\headheight}{14pt}
\cfoot{\thepage}

% Nice abstract
\renewenvironment{abstract}
{\small
	\begin{center}
		\bfseries \abstractname\vspace{-.5em}\vspace{0pt}
	\end{center}
	\list{}{
		\setlength{\leftmargin}{.5cm}%
		\setlength{\rightmargin}{\leftmargin}%
	}%
	\item\relax}
{\endlist}



\begin{document}

% Title
\title{
	Applying Learning Classifier Systems to Acoustic Scene Classification: DCASE 2017 Challenge \\
	\vspace{0.2in}
	\large CITS4404 Artificial Intelligence \& Adaptive Systems Team Project
}
\author{Yiyang~Gao~(21263128)}
\author{Aaron~Hurst~(21325887)}
\author{Kevin~Kuek~(21307006)}
\author{Scott~McCormack~(21875529)}
\affil{School of Computer Science and Software Engineering}

\date{3rd November, 2017}

\maketitle

% Abstract
\begin{abstract}
	This will be our abstract \\
	\lipsum[2]
\end{abstract}

\begin{multicols}{2}


\section{Introduction}


\lipsum[1]




\section{Background}

This section provides a brief review of learning classifier systems (\ref{sec:LCS}), the DCASE Challenge (\ref{sec:DCASE}) and acoustic scene classification (\ref{sec:ASC}).



\subsection{Learning Classifier Systems}
\label{sec:LCS}

First introduced in the mid-1970s, Learning Classifier Systems (LCSs) are a rule-based machine leaning algorithm with a unique combination of learning mechanisms \cite{Butz2015}. The core of LCSs is the population of rules, or \textit{classifiers}, which collectively form the solution to the given problem \cite{Urbanowicz2009}. This population of classifiers is gradually evolved toward an optimal and \textit{optimally general} set \cite{Urbanowicz2009}.

The motivation for this structure is that, when modelling and attempting to predict the outcome of complex systems, a desirable approach is to develop a distributed population of classifiers -- or rules -- that together form an accurate model \cite[p.~2]{Urbanowicz2009}. Each classifier, then, spans a subspace of the problem, with the population spanning the entire problem.

Individual classifiers consist of a condition-action rule which says: 'If a problem instance matches this condition, perform this action'.


, an action and a number of parameters. The condition specifies the subspace of the problem, while the action proposes an outcome for this subspace. For a given \textit{instance} of the problem, the classifiers then say: `If the problem instance matches


XCS can be distinguished by the
following key features: an accuracy based fitness, a niche
GA (acting in the action set [A]), and an adaptation of
standard Q-Learning as credit assignment.




evolved using a learning mechanism incorporating a number of mechanisms -- GA, subsumption, deletion and covering

online vs reinforcement (key success area of LCS)

continuous vs discrete data

fitness: strength vs. accuracy

Common variants: MCS, XCS, XCSR




\subsection{DCASE Challenge}
\label{sec:DCASE}

what (sound recognition, machine listening), why (motivations) and how (mechanics of challenge)

baseline solution, best results


\subsection{Acoustic Scene Classification}
\label{sec:ASC}

more detailed description of this particular task from the challenge

\section{Features}
\label{sec:feat}
\subsection{Feature Engineering}
Feature engineering is a vital aspect of machine learning, as the way that data is presented to a predictive model has a large impact on the quality of results achieved \cite{Brownlee2014}. This is because features that don’t capture important information from the dataset effectively will not provide an accurate representation of the environment and may ignore useful patterns. Feature extraction is a method of deriving key information from a dataset into a useful format by using tools specific to the problem domain and typically results in a reduction in data dimensionality \cite{Howbert2012}. The “Curse of Dimensionality” is an important concept in the domain of machine learning that refers to the problem that as the dimensions of feature set increases, the volume of the feature space increases exponentially \cite{Keogh2010}. This has a particularly large impact on LCSs, as, with the exception of wildcards, the rule for every single feature is required to match with that of the instance to classify. To illustrate this, for a feature set of size 1000 and a classifier with 50\% wildcards, still 500 features from the environment instance would need to perfectly match the classifier’s corresponding rules and if even one of them does not, the instance will not be matched. One way to interpret this is that for a highly dimensional feature set each individual feature is insignificant \cite{Keogh2010}, but can have a large impact on the result.
For the application of acoustic scene classification, defining features need to be extracted from sound files, and these need to be reduced to a dimensionality suitable for use in LCS rules.

\subsection{Feature Extraction}
The mel scale is a scale of pitches that was designed to vary linearly with a listener’s perception of a sound and is used in the LCS for sound data representation. It is an informal unit of measure that can be converted to from the frequency spectrum by using a function derived from what listeners judge to be pitches of equal distance from each other \cite{Luening1975}. There are various ways of converting from Hz to mels and a common method uses the equation  \cite{OShaughnessy1987}, a plot of which can be seen in 
\todo[inline]{reference hz2mel.png}. 
Acoustic classifiers often convert frequencies to the mel scale because its characteristic of mimicking human perception, and due to its logarithmic transformation of the frequency spectrum, it also results in dimension reduction when using bands as features \cite{Stowell2014}.
\todo[inline]{--- hz2mel here ---}
Initial feature extraction was done using the DCASE Baseline system’s included feature extractor, which makes use of python music and audio analysis package called LibROSA \cite{Heittola2017}. The feature extractor takes an audio file as an input and outputs an array in which each row is a time slice and each column is a feature, with the specifics dependent on a configurable parameters list. The parameters used can be seen in 
\todo[inline]{reference parameters figure}. 
Based on the settings used, the feature extractor output arrays with 501 time slices and 40 features, with each feature being the log of the magnitude of a band on the mel scale. This results in a feature set for each sound file of  dimensions.

\todo[inline]{--- table here ---}
\begin{table}
\caption{DCASE Baseline feature extractor parameters used}
\label{label}
	\begin{tabular}{|l|l|}
		\hline
		Parameter 						& Value used \\ \hline
		Maximum frequency for calculating MEL band	& 22050 \\
		Minimum frequency for calculating MEL band	& 0	\\		
		Sample frequency				& 44100\\
		Hop length (samples)			&	882\\
		Hop length (seconds)			& 0.02\\
		Use htk style mel conversion	& false\\
		Use log scale					& true\\
		Feature extraction method		& MEL\\
		Average multichannel audio to single channel & true\\
		FFT length						& 2048\\
		Number of MEL bands				& 40\\
		Normalise MEL bands				& false\\
		Type of spectrogram				& magnitude\\
		Window length (samples)			& 1764\\
		Window length (seconds)			& 0.04\\
		Window type						& Hamming asymmetric
	\end{tabular}
\end{table}

\subsection{Feature Reduction}
The feature set extracted using the baseline system has a very high dimension, and in that form would be unsuitable for use with an LCS, as a feature set of that size would cause increased processing time as well as classifiers that are overfitted to specific instances. In order to perform dimensionality reduction without losing too much important information about that datasets, some visualisations were made to allow visual inspection and identification of trends to potentially be exploited. Each of the graphs shown in 
\todo[inline]{reference beachgrid.png, cargrid.png, officegrid.png} 
show the features extracted for a given sound file, with 9 different sound files of a given classification displayed together. Each line represents one of the 501 time slices and the magnitude of each mel band in that slice. 
\todo[inline]{---3 grids here in 1 figure---}
Comparing these graphs, it is clear that there are patterns common to individual classifications and that in a given sound file, the distribution of mel band magnitudes follows a similar pattern for many of the time slices. Given this similarity between time slices, it was decided that the mean and standard deviation of each mel band magnitude across all time slices would be used as the feature set, as this reduces the dimensionality by 250 times while maintaining a measure of both the trend and spread of the data. The three example classes shown in 
\todo[inline]{reference beachgrid.png, cargrid.png, officegrid.png} 
are shown again in 
\todo[inline]{reference beach.png, car.png, office.png}, 
with the difference that each line represents the average log magnitude of for each mel band in one sound file of the class shown rather than each line being one time slice of a single file. These graphs illustrate that by taking the mean magnitude for each mel band, the general trends are preserved, and though the spreading of these values between time slices is not represented, it is captured in the standard deviations for each mel band. The average mean and standard per mel band across all sound files of each example classification is shown in 
\todo[inline]{reference 3means.png, 3stds.png} and illustrates that the differences between classifications is still captured by the chosen features despite the dimension reduction.
\todo[inline]{---3 graphs (beach car and office.png) here in 1 fig---}
\todo[inline]{---2 graphs (3means and 3stds (lol)) in 1 fig---}
The amount of spread between different instances of each class observed in 
\todo[inline]{reference beach.png, car.png, office.png}
is noticeably large and would result in classifiers needing to have a large range of accepted values in order to catch many other instances of their classification. This spreading is potentially due to the sound files having different recorded volumes which may result in magnitude offset observed. In attempt to rectify this, the means were normalised such that in a given sound file, instead of using the means as features, the average across the means of each mel band was taken and the features used were the mel band means subtract the average mean. This method would make magnitude offset irrelevant while maintaining trends and is illustrated in 
\todo[inline]{reference normalise.png} 
with the results displayed in 
\todo[inline]{reference beachVar.png, carVar.png, officeVar.png} 
confirming that the general trends are preserved while reducing spread on the y axis. Unfortunately, upon testing this method was found to reduce classification accuracy so it was not used for further testing. Further analysis of the feature set showed that this is likely due to the offset of the mel band magnitudes being a defining characteristic of different scenes, which can be observed in 
\todo[inline]{reference allMeans.png, allMeanVars.png} 
in which the average means of each class are plotted on one graph and the average normalised means are plotted on another. The graph of average means shows that classes are distinct from one another based on their magnitude offset and the other shows that by normalising them this feature is lost. The reduction in accuracy may imply that either the difference in trend was not enough to make classes distinct from one another or potentially that more parameter tuning of the LCS would be needed to take advantage of the normalisation.
\todo[inline]{---normalise.png here---}
\todo[inline]{---beachVar, carVar and officeVar.png here---}
\todo[inline]{---allMeans and allMeanVars.png here---}

\section{Experiments}
\label{sec:exp}

blah

\subsection{Modification of Existing Code}
\label{sec:ryan}

\todo[inline]{Is there a better title you'd suggest Scott?}

Description of Ryan's code, how we modified it and the results obtained



\subsection{Adapted XCS(R) Design}
\label{sec:home}

Details of the LCS we made, specific design changes relative to the standard implementation for our problem




%- Feature Extraction, motivation for feature choices (i.e. feautres don't vary a lot across a file (low standard deviation), need to reduce the number of features)
%
%- Description of the code (the one we made outselved and Urbaonwicz's)
%
%- Parameters used
%
%Structure and design of algorithm: generic XCS modified for continuous data and offline learning
%
%Key modifications to basic LCs: (1) added tolerance to subsumption and removed wildcard criteria so that subsumption has some likelihood of occurring, (2) changed deletion criteria so that classifiers with little utility (low match count) can be deleted, motivation was the really low average match count, (3) action set subsumption criteria for most general classifier determined as most wildcards or equal most and largest range, (4) ...
%
%Show graph of function for deletion experience threshold (developed in response to \#2)




\section{Results}


\subsection{Environment Representations}

Alternative feature processing investigated



\subsection{Parameter Tuning}

%- Rate of learning (improvement in accuracy over time)
%
%- Overall results: pairwise, all classes at once (confusion matrices)
%
%
%Theoretical analysis?






\section{Discussion}








\section{Conclusion}









\bibliographystyle{IEEEtranN}
\bibliography{IEEEabrv,References}

\end{multicols}

\end{document}