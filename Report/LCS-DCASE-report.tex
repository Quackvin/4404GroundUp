\documentclass[11pt]{article}

% Packages
\usepackage[affil-it]{authblk}		% author affiliations in title
\usepackage[margin=1in]{geometry}	% one inch margins
\usepackage{enumerate}				% numbered list environment
\usepackage{wrapfig}				% text wrapped figures
\usepackage{graphicx}				% figures, better than "graphics" apparently
\usepackage{subcaption}				% captions for sub-figures
\usepackage{amsmath}				% math stuff
\usepackage{fancyhdr}				% custom headers
\usepackage[numbers]{natbib}		% used for citet and other citation formats
\usepackage{booktabs}				% nice table borders
\usepackage{tabularx}				% equal width table columns
\usepackage[hidelinks]{hyperref} 	% puts click-able links in the text, fixes issue with urls that have underscores
\usepackage[defaultlines=2,all]{nowidow}	% prevents orphan and widow lines at start and end of paragraphs
\usepackage{multicol}				% control over using multiple columns
\usepackage{lipsum}					% dummy text

% To-do notes and to-do list
\usepackage[colorinlistoftodos,prependcaption]{todonotes}

% Width between columns
\setlength{\columnsep}{0.8cm}

% Tables with centred, fixed with columns
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}		% used in cover page
\newcolumntype{Y}{>{\centering\arraybackslash}X}			% used in tabularx for even column distribution

% Make "References" appear in the table of contents
\usepackage[nottoc]{tocbibind}
\renewcommand{\tocbibname}{References}

% Headers and footers
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rhead{CITS4404 Artificial Intelligence \& Adaptive Systems}
\lhead{Team G1}
\setlength{\headheight}{14pt}
\cfoot{\thepage}

% Nice abstract
\renewenvironment{abstract}
{\small
	\begin{center}
		\bfseries \abstractname\vspace{-.5em}\vspace{0pt}
	\end{center}
	\list{}{
		\setlength{\leftmargin}{.5cm}%
		\setlength{\rightmargin}{\leftmargin}%
	}%
	\item\relax}
{\endlist}



\begin{document}

% Title
\title{
	Applying Learning Classifier Systems to Acoustic Scene Classification: DCASE 2017 Challenge \\
	\vspace{0.2in}
	\large CITS4404 Artificial Intelligence \& Adaptive Systems Team Project
}
\author{Yiyang~Gao~(21263128)}
\author{Aaron~Hurst~(21325887)}
\author{Kevin~Kuek~(21307006)}
\author{Scott~McCormack~(21875529)}
\affil{School of Computer Science and Software Engineering}

\date{3rd November, 2017}

\maketitle

% Abstract
\begin{abstract}
	This will be our abstract \\	
	\lipsum[2]
\end{abstract}

\begin{multicols}{2}


\section{Introduction}


\lipsum[1]




\section{Background}

This section provides a brief review of learning classifier systems (\ref{sec:LCS}), the DCASE Challenge (\ref{sec:DCASE}) and acoustic scene classification (\ref{sec:ASC}).



\subsection{Learning Classifier Systems}
\label{sec:LCS}

First introduced in the mid-1970s, Learning Classifier Systems (LCSs) are a rule-based machine leaning algorithm with a unique combination of learning mechanisms \cite{Butz2015}. The core of LCSs is the population of rules, or \textit{classifiers}, which collectively form the solution to the given problem \cite{Urbanowicz2009}. This population of classifiers is gradually evolved toward an optimal and \textit{optimally general} set \cite{Urbanowicz2009}.

The motivation for this structure is that, when modelling and attempting to predict the outcome of complex systems, a desirable approach is to develop a distributed population of classifiers -- or rules -- that together form an accurate model \cite[p.~2]{Urbanowicz2009}. Each classifier, then, spans a subspace of the problem, with the population spanning the entire problem. Individual classifiers consist of a condition-action rule which says: 'If a problem instance matches this condition, perform this action'. Classifier fitness is evaluated based on feedback from the problem (also known as the environment).

The learning process of a LCS includes a rule discovery method known as covering, a generalisation-pressure effect called subsumption, a generic algorithm (GA), fitness-based deletion to maintain a finite-sized population and, in some cases, reinforcement learning \cite{Butz2000}.

The rule discovery method, covering, is used to initialise the population by adding a new classifier whenever a problem instance matches no existing classifier. Subsumption is used to eliminate classifiers with more specific conditions that are no more accurate than others with equivalent, but more general conditions. The GA is used as a secondary rule discovery method that only operates on high performing rules. Deletion is employed to maintain a finite population size by removing poorly performing classifiers. Reinforcement learning may be used as a final step in the learning cycle for problems where feedback from the environment is delayed.

A seminal work in the field of LCSs was the introduction of the eXtended Classifier System by Wilson \cite{Lanzi2008,Sigaud2007}. This incorporated a number of features which substantially improved the performance of the LCS \cite{Lanzi2008}.

A key distinction amongst LCS applications is between supervised learning, in which feedback from the environment is delayed (such as robot navigation), and offline learning, where feedback is immediate and the correct action known in advance (such as classification tasks). This distinction determines whether reinforcement learning is necessary and affects how classifier accuracy is calculated.



\subsection{DCASE Challenge}
\label{sec:DCASE}

Sound classification, or machine listening, is seen as a promising research field with multifarious applications \cite{Mesaros2017}. The DCASE challenge has been established to encourage and support work in this space by providing standardised 



what (sound recognition, machine listening), why (motivations) and how (mechanics of challenge)

baseline solution, best results


\subsection{Acoustic Scene Classification}
\label{sec:ASC}

more detailed description of this particular task from the challenge



\section{Experiments}
\label{sec:exp}

blah

\subsection{Modification of Existing Code}
\label{sec:ryan}

\todo[inline]{Is there a better title you'd suggest Scott?}

Description of Ryan's code, how we modified it and the results obtained





\subsection{Adapted XCS(R) Design}
\label{sec:home}

Details of the LCS we made, specific design changes relative to the standard implementation for our problem


\[
\frac{\textrm{deletionThreshold}}{3.05}
\left(
\pi/2-\arctan\left(\frac{\textrm{age} - \textrm{ageThreshold}}{20} \right) \right)
\]



%- Feature Extraction, motivation for feature choices (i.e. feautres don't vary a lot across a file (low standard deviation), need to reduce the number of features)
%
%- Description of the code (the one we made outselved and Urbaonwicz's)
%
%- Parameters used
%
%Structure and design of algorithm: generic XCS modified for continuous data and offline learning
%
%Key modifications to basic LCs: (1) added tolerance to subsumption and removed wildcard criteria so that subsumption has some likelihood of occurring, (2) changed deletion criteria so that classifiers with little utility (low match count) can be deleted, motivation was the really low average match count, (3) action set subsumption criteria for most general classifier determined as most wildcards or equal most and largest range, (4) ...
%
%Show graph of function for deletion experience threshold (developed in response to \#2)




\section{Results}


\subsection{Environment Representations}

Alternative feature processing investigated



\subsection{Parameter Tuning}

%- Rate of learning (improvement in accuracy over time)
%
%- Overall results: pairwise, all classes at once (confusion matrices)
%
%
%Theoretical analysis?



Reason why standard deviation and mean are used, what information they carry




\section{Discussion}




Car and office confusion matrix
Plot for std dev and mean (for car and office) to explain
Car, office and city centre (with confusion and plots)
Then add metro station and show results (worse) - explain why it is worse
Show a classifier on a graph to show how it can match instances
Overall accuracy (all 15 features)




\section{Conclusion}









\bibliographystyle{IEEEtranN}
\bibliography{IEEEabrv,References}

\end{multicols}

\end{document}